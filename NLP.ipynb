{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4def187b-1aa5-4b1a-95ec-7483b0888b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks. From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything. In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ed4eb63-cbd9-4155-9229-bdf73b052131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "985400fc-901f-438e-97d3-144453d407b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04572e9e-bb58-4bbf-ad1d-64848cf74dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'one', 'of', 'the', 'largest', 'Python', 'libraries', 'for', 'performing', 'various', 'Natural', 'Language', 'Processing', 'tasks', '.', 'From', 'rudimentary', 'tasks', 'such', 'as', 'text', 'pre-processing', 'to', 'tasks', 'like', 'vectorized', 'representation', 'of', 'text', '–', 'NLTK', '’', 's', 'API', 'has', 'covered', 'everything', '.', 'In', 'this', 'article', ',', 'we', 'will', 'accustom', 'ourselves', 'to', 'the', 'basics', 'of', 'NLTK', 'and', 'perform', 'some', 'crucial', 'NLP', 'tasks', ':', 'Tokenization', ',', 'Stemming', ',', 'Lemmatization', ',', 'and', 'POS', 'Tagging', '.']\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f22db75a-a3c0-465f-b9cd-a3e9ee633ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c917026-affe-4eda-b7ba-40e30cae8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Ak114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b439dd57-ae53-489f-8873-7e3c8edf2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pos_tag(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3adc9f02-07d5-42ac-baa1-ccda5acf6f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Toolkit', 'NNP'), ('(', '('), ('NLTK', 'NNP'), (')', ')'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('largest', 'JJS'), ('Python', 'NNP'), ('libraries', 'NNS'), ('for', 'IN'), ('performing', 'VBG'), ('various', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('tasks', 'NNS'), ('.', '.'), ('From', 'IN'), ('rudimentary', 'JJ'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('text', 'JJ'), ('pre-processing', 'NN'), ('to', 'TO'), ('tasks', 'NNS'), ('like', 'IN'), ('vectorized', 'JJ'), ('representation', 'NN'), ('of', 'IN'), ('text', 'JJ'), ('–', 'NNP'), ('NLTK', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('API', 'NNP'), ('has', 'VBZ'), ('covered', 'VBN'), ('everything', 'NN'), ('.', '.'), ('In', 'IN'), ('this', 'DT'), ('article', 'NN'), (',', ','), ('we', 'PRP'), ('will', 'MD'), ('accustom', 'VB'), ('ourselves', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('basics', 'NNS'), ('of', 'IN'), ('NLTK', 'NNP'), ('and', 'CC'), ('perform', 'VB'), ('some', 'DT'), ('crucial', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), (':', ':'), ('Tokenization', 'NN'), (',', ','), ('Stemming', 'NNP'), (',', ','), ('Lemmatization', 'NNP'), (',', ','), ('and', 'CC'), ('POS', 'NNP'), ('Tagging', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b352f7-658d-4537-af6f-62c54bf581b2",
   "metadata": {},
   "source": [
    "# TOKENIZATION\n",
    "1. Sentence Tokenization\n",
    "2. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0da75338-ab09-4a0b-8a6c-39ff68cc62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks. From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything. In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "973eb280-b98c-44be-a8e5-cb31440095bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks. From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything. In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cf7d86ec-009e-4bbd-97f9-7b635c69c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09b8ed6e-5f30-45b4-8d51-5771a5924575",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30206888-aac6-437e-9aa9-a542d6a181be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks.',\n",
       " 'From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything.',\n",
       " 'In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "75ed3cce-3b4d-4b7c-91e3-f57aabcf8523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks. \n",
      "\n",
      "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything. \n",
      "\n",
      "In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sent:\n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "95a7ee92-d20c-4205-8b6e-0b61e1415764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'one', 'of', 'the', 'largest', 'Python', 'libraries', 'for', 'performing', 'various', 'Natural', 'Language', 'Processing', 'tasks', '.', 'From', 'rudimentary', 'tasks', 'such', 'as', 'text', 'pre-processing', 'to', 'tasks', 'like', 'vectorized', 'representation', 'of', 'text', '–', 'NLTK', '’', 's', 'API', 'has', 'covered', 'everything', '.', 'In', 'this', 'article', ',', 'we', 'will', 'accustom', 'ourselves', 'to', 'the', 'basics', 'of', 'NLTK', 'and', 'perform', 'some', 'crucial', 'NLP', 'tasks', ':', 'Tokenization', ',', 'Stemming', ',', 'Lemmatization', ',', 'and', 'POS', 'Tagging', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3425ba-020a-461a-80ff-2b1d5140fe18",
   "metadata": {},
   "source": [
    "# Removing Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3d4c90d2-4957-448b-8647-04a8b017df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"As described previously, a token is a piece of data that stands in for another, more valuable piece of information. Tokens have virtually no value on their own they are only useful because they represent something valuable, such as a credit card primary account number (PAN) or Social Security number (SSN).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "27fd3011-089c-4306-b1cf-932503817db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As described previously, a token is a piece of data that stands in for another, more valuable piece of information. Tokens have virtually no value on their own they are only useful because they represent something valuable, such as a credit card primary account number (PAN) or Social Security number (SSN).'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a39ed761-8f4f-4316-8608-251802e5c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a229149a-fc81-4166-8c98-d8e60e71edd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As',\n",
       " 'described',\n",
       " 'previously',\n",
       " ',',\n",
       " 'a',\n",
       " 'token',\n",
       " 'is',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'data',\n",
       " 'that',\n",
       " 'stands',\n",
       " 'in',\n",
       " 'for',\n",
       " 'another',\n",
       " ',',\n",
       " 'more',\n",
       " 'valuable',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'information',\n",
       " '.',\n",
       " 'Tokens',\n",
       " 'have',\n",
       " 'virtually',\n",
       " 'no',\n",
       " 'value',\n",
       " 'on',\n",
       " 'their',\n",
       " 'own',\n",
       " 'they',\n",
       " 'are',\n",
       " 'only',\n",
       " 'useful',\n",
       " 'because',\n",
       " 'they',\n",
       " 'represent',\n",
       " 'something',\n",
       " 'valuable',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'a',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'primary',\n",
       " 'account',\n",
       " 'number',\n",
       " '(',\n",
       " 'PAN',\n",
       " ')',\n",
       " 'or',\n",
       " 'Social',\n",
       " 'Security',\n",
       " 'number',\n",
       " '(',\n",
       " 'SSN',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "56f77f7d-6a4f-4a8a-8880-eea53b0b3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "62a35a32-4459-4634-8779-ab88fb2a413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4dca069d-56f3-464a-9cfc-e48661aa436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = list(punctuation) + stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "741a4380-6792-428e-a8ec-1118490dea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9fa148eb-58bb-420a-aad5-01b547f262de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As\n",
      "described\n",
      "previously\n",
      "token\n",
      "piece\n",
      "data\n",
      "stands\n",
      "another\n",
      "valuable\n",
      "piece\n",
      "information\n",
      "Tokens\n",
      "virtually\n",
      "value\n",
      "useful\n",
      "represent\n",
      "something\n",
      "valuable\n",
      "credit\n",
      "card\n",
      "primary\n",
      "account\n",
      "number\n",
      "PAN\n",
      "Social\n",
      "Security\n",
      "number\n",
      "SSN\n"
     ]
    }
   ],
   "source": [
    "for i in new_string:\n",
    "    if i not in stop_word:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86626d5d-d467-40f1-b45d-93d60a6423b4",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9368a705-d7fd-4e24-894e-5e8da7f8eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,RegexpStemmer,SnowballStemmer,LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e0919cc-7bc1-4481-80e2-0055d74b0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()\n",
    "r = RegexpStemmer('ing')\n",
    "s = SnowballStemmer('english')\n",
    "l = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb11e010-fb52-43b2-bb69-5f1f35eeb8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.stem(\"Changing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "01a991b6-d97f-4ef3-9d3c-6f116f0aa1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"Changing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f9eaf3ae-b95e-419f-9bf1-5b30f58eba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changed'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.stem(\"Changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd6df978-1809-4ffe-86c0-21f99481aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.stem(\"Change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecb59c-3a95-460a-b81f-874b0bc25b4a",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1f59c6eb-933c-4866-b5d1-e7ee8d8e25a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ak114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6260f0d6-c8d3-4b3d-901b-f38c03650623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "be2a4116-bb1a-4dac-aa1f-770ead62f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebc4d4b3-16eb-4569-a2dc-85478372f7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "37849856-7d32-4e62-aa9e-e8f0d527195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"mice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5d147d9a-0755-4710-846c-8f929f422839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"mouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91783a-cb88-4cac-afa5-9550a17543b2",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e89a4ff4-fc60-408f-9164-5120dc719c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"Hi i am ashish i am studet i am tech men i am good men i am tech \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f6df73dc-594f-4dda-9597-a33307000a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi i am ashish i am studet i am tech men i am good men i am tech '"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "faae7782-e09d-43a1-97f6-05f6d22438c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eb331f4e-ffb6-4993-84c6-1956ad774487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ak114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4a0db13d-a0a4-459a-99ef-5be51d4424b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'i',\n",
       " 'am',\n",
       " 'ashish',\n",
       " 'i',\n",
       " 'am',\n",
       " 'studet',\n",
       " 'i',\n",
       " 'am',\n",
       " 'tech',\n",
       " 'men',\n",
       " 'i',\n",
       " 'am',\n",
       " 'good',\n",
       " 'men',\n",
       " 'i',\n",
       " 'am',\n",
       " 'tech']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(var)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bb36e4f5-5941-45a9-9f6e-79e376f15aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6ad98e4e-9f54-4fa0-a0d6-ba4cf520bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BigramCollocationFinder.from_words(words)\n",
    "t = TrigramCollocationFinder.from_words(words)\n",
    "n = ngrams(words,3)  # you can change it like 1,2,3,4.............words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1aeef91d-9874-4cde-84fd-2deab1c1cc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('i', 'am'): 5, ('am', 'tech'): 2, ('men', 'i'): 2, ('Hi', 'i'): 1, ('am', 'ashish'): 1, ('ashish', 'i'): 1, ('am', 'studet'): 1, ('studet', 'i'): 1, ('tech', 'men'): 1, ('am', 'good'): 1, ...})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram\n",
    "b.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "57dff3e9-6954-45d4-99e0-f3338ce3ed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('Hi', 'i'), ('i', 'am'), ('am', 'ashish'), ('ashish', 'i'), ('am', 'studet'), ('studet', 'i'), ('am', 'tech'), ('tech', 'men'), ('men', 'i'), ('am', 'good'), ('good', 'men')])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.ngram_fd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0d1801cd-546a-4d8b-9a0e-ddda979cb775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('i', 'am', 'tech'): 2, ('men', 'i', 'am'): 2, ('Hi', 'i', 'am'): 1, ('i', 'am', 'ashish'): 1, ('am', 'ashish', 'i'): 1, ('ashish', 'i', 'am'): 1, ('i', 'am', 'studet'): 1, ('am', 'studet', 'i'): 1, ('studet', 'i', 'am'): 1, ('am', 'tech', 'men'): 1, ...})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigram\n",
    "t.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4db601c6-5bae-4929-bbb4-7dc56361cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi', 'i', 'am')\n",
      "('i', 'am', 'ashish')\n",
      "('am', 'ashish', 'i')\n",
      "('ashish', 'i', 'am')\n",
      "('i', 'am', 'studet')\n",
      "('am', 'studet', 'i')\n",
      "('studet', 'i', 'am')\n",
      "('i', 'am', 'tech')\n",
      "('am', 'tech', 'men')\n",
      "('tech', 'men', 'i')\n",
      "('men', 'i', 'am')\n",
      "('i', 'am', 'good')\n",
      "('am', 'good', 'men')\n",
      "('good', 'men', 'i')\n",
      "('men', 'i', 'am')\n",
      "('i', 'am', 'tech')\n"
     ]
    }
   ],
   "source": [
    "# N-Gram\n",
    "for i in n:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878acfb-66e3-45c5-8b25-f51be0cb2eee",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "207c107d-bfbc-46f1-94d0-ecbcfa2229a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "83bd04cd-215d-4862-a4ff-8ff34cb3550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"Hi Ashish\",\"How Are You\",\"What is the next plan\",\"Tom is a cat of jerry\",\"Today is holiday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f1d086d3-33b9-4d02-a4ed-2372a504259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"About\":l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d08bf68b-9ae8-432a-b542-ef3046e8f3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>About</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Ashish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Are You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the next plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom is a cat of jerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is holiday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   About\n",
       "0              Hi Ashish\n",
       "1            How Are You\n",
       "2  What is the next plan\n",
       "3  Tom is a cat of jerry\n",
       "4       Today is holiday"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ab420ec0-2dbb-4f83-9b38-d0b398608aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2ba4d075-8646-422d-94fb-af765a4e4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fb912c61-e55f-4b83-b2c6-3a08399a4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = cv.fit_transform(df[\"About\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ddbc56b2-7af6-455f-b15b-c20a000cc1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "14a6252a-3cf9-403f-bae1-ba97c122677e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': 3,\n",
       " 'ashish': 1,\n",
       " 'how': 5,\n",
       " 'are': 0,\n",
       " 'you': 15,\n",
       " 'what': 14,\n",
       " 'is': 6,\n",
       " 'the': 11,\n",
       " 'next': 8,\n",
       " 'plan': 10,\n",
       " 'tom': 13,\n",
       " 'cat': 2,\n",
       " 'of': 9,\n",
       " 'jerry': 7,\n",
       " 'today': 12,\n",
       " 'holiday': 4}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f433b-d6d2-456c-9013-faa40d69bb11",
   "metadata": {},
   "source": [
    "# Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cea40adf-f2ff-451e-8891-ede4add12302",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = \"Computer science is the study of the structure, transformation, and limits of information. It encompasses the design, development, and analysis of algorithms, as well as the creation of software and hardware systems that process, store, and communicate information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2614aa79-f8b4-4447-bc7e-52544b961e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computer science is the study of the structure, transformation, and limits of information. It encompasses the design, development, and analysis of algorithms, as well as the creation of software and hardware systems that process, store, and communicate information.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "163cf08e-3c2a-4069-811a-b8881e3c2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d47ba126-c8f2-4d13-b354-a0ac40b15cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comp = lesk(word_tokenize(comp),\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "de4e98c4-9f0d-4d05-b359-d5cece0a1403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('computer.n.01')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fc10bb8a-b212-467e-9e2c-f49c5e5696b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a machine for performing calculations automatically'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comp.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d420f0f-1f88-4222-b055-c1c463ce85f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d66fbc-136c-4443-a7bd-eb61249d658f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
